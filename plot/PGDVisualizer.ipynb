{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.chdir(\"..\")\n",
    "os.makedirs(\"./cache\", exist_ok=True)\n",
    "\n",
    "from datasets import ImageNet_LT\n",
    "from models import *\n",
    "from trainer import load_clip_to_cpu\n",
    "from utils.evaluator import PGD\n",
    "\n",
    "from utils.config import _C as cfg\n",
    "\n",
    "cfg.defrost()\n",
    "cfg.merge_from_file('./configs/data/imagenet_lt.yaml')\n",
    "cfg.merge_from_file('./configs/model/clip_vit_b16.yaml')\n",
    "cfg.adaptformer = True\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    device = torch.device(\"cpu\")\n",
    "elif cfg.gpu is None:\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    torch.cuda.set_device(cfg.gpu)\n",
    "    device = torch.device(\"cuda:{}\".format(cfg.gpu))\n",
    "\n",
    "cfg.model_dir = './output/imagenet_lt_clip_vit_b16_adaptformer_True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg.backbone.startswith(\"CLIP\"):\n",
    "    mean = [0.48145466, 0.4578275, 0.40821073]\n",
    "    std = [0.26862954, 0.26130258, 0.27577711]\n",
    "else:\n",
    "    mean = [0.5, 0.5, 0.5]\n",
    "    std = [0.5, 0.5, 0.5]\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize(cfg.resolution * 8 // 7),\n",
    "    transforms.CenterCrop(cfg.resolution),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std),\n",
    "])\n",
    "\n",
    "train_dataset = ImageNet_LT(root=cfg.root, train=True, transform=transform_test)\n",
    "num_classes = train_dataset.num_classes\n",
    "cls_num_list = train_dataset.cls_num_list\n",
    "test_dataset = ImageNet_LT(root=cfg.root, train=False, transform=transform_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=False, num_workers=8, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=8, pin_memory=True)\n",
    "\n",
    "clip_model = load_clip_to_cpu(cfg.backbone, cfg.prec)\n",
    "model = PeftModelFromCLIP(cfg, clip_model, num_classes)\n",
    "model.to(device)\n",
    "tuner = model.tuner\n",
    "head = model.head\n",
    "\n",
    "load_path = os.path.join(cfg.model_dir, \"checkpoint.pth.tar\")\n",
    "checkpoint = torch.load(load_path, map_location=device)\n",
    "tuner_dict = checkpoint[\"tuner\"]\n",
    "head_dict = checkpoint[\"head\"]\n",
    "tuner.load_state_dict(tuner_dict)\n",
    "head.load_state_dict(head_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_images(original_images, adversarial_images, labels, adv_labels, class_names):\n",
    "    \"\"\"\n",
    "    Visualizes original and adversarial images side by side.\n",
    "    \"\"\"\n",
    "    batch_size = original_images.size(0)\n",
    "    for i in range(batch_size):\n",
    "        original = original_images[i].permute(1, 2, 0).cpu().numpy()\n",
    "        adversarial = adversarial_images[i].permute(1, 2, 0).cpu().numpy()\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "        axes[0].imshow((original * std + mean).clip(0, 1))\n",
    "        axes[0].set_title(f\"Original: {class_names[labels[i].item()]}\")\n",
    "        axes[0].axis(\"off\")\n",
    "\n",
    "        axes[1].imshow((adversarial * std + mean).clip(0, 1))\n",
    "        axes[1].set_title(f\"Adversarial: {class_names[adv_labels[i].item()]}\")\n",
    "        axes[1].axis(\"off\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 8 / 255  # 攻击强度\n",
    "alpha = 2 / 255  # 每步扰动量\n",
    "num_iterations = 2  # 攻击迭代次数\n",
    "\n",
    "model.eval()  # 切换到评估模式\n",
    "class_names = test_dataset.classnames  # 类别名称\n",
    "\n",
    "for images, labels in test_loader:\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "    \n",
    "    # 原始预测\n",
    "    with torch.no_grad():\n",
    "        original_logits = model(images)\n",
    "        original_preds = torch.argmax(original_logits, dim=1)\n",
    "\n",
    "    # 对抗样本生成\n",
    "    adversarial_images = PGD(images, labels, model, eps=epsilon, alpha=alpha, steps=num_iterations)\n",
    "    \n",
    "    # 对抗样本预测\n",
    "    with torch.no_grad():\n",
    "        adversarial_logits = model(adversarial_images)\n",
    "        adversarial_preds = torch.argmax(adversarial_logits, dim=1)\n",
    "\n",
    "    # 可视化前10张图片\n",
    "    visualize_images(\n",
    "        original_images=images[:10],\n",
    "        adversarial_images=adversarial_images[:10],\n",
    "        labels=original_preds[:10],\n",
    "        adv_labels=adversarial_preds[:10],\n",
    "        class_names=class_names\n",
    "    )\n",
    "    \n",
    "    break  # 仅显示一批数据\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_under_attack(model, data_loader, epsilon, alpha, num_iter):\n",
    "    \"\"\"\n",
    "    Evaluates model accuracy under PGD attack.\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in tqdm(data_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # 生成对抗样本\n",
    "        adversarial_images = PGD(model, images, labels, eps=epsilon, alpha=alpha, num_iter=num_iter)\n",
    "\n",
    "        # 预测\n",
    "        with torch.no_grad():\n",
    "            outputs = model(adversarial_images)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Accuracy under PGD attack (ε={epsilon}): {accuracy:.2f}%\")\n",
    "    return accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 评估模型在对抗样本上的性能\n",
    "evaluate_model_under_attack(model, test_loader, epsilon, alpha, num_iterations)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LT-Survey",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
